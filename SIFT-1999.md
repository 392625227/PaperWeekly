# Object Recognition from Local Scale-Invariant Features

David G. Lowe University of British Columbia, Vancouver, Canada

## 0. Abstract

An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.

提出了一种使用新的局部图像特征类别的目标识别系统。特征对于图像缩放、平移和旋转是不变的，对于光照变化和仿射或3D投影是部分不变的。这些特征与下颞皮质中的神经元有类似的性质，这是用于目标识别的，在灵长类视觉区。通过分阶段的滤波方法，可以高效的检测特征，在尺度空间中识别出稳定的点。创建了图像键，可以进行局部几何形变，在多个方向平面和多个尺度上表示模糊的图像梯度。这些键用作最近邻索引方法的输入，可以识别出候选的目标匹配。进一步的对每个匹配进行验证，可以通过对未知的模型参数找到一个低残差的最小二乘解。试验结果表明，在群聚的部分遮挡的图像中，可以得到稳健的目标识别结果，耗时低于2秒。

## 1. Introduction

Object recognition in cluttered real-world scenes requires local image features that are unaffected by nearby clutter or partial occlusion. The features must be at least partially invariant to illumination, 3D projective transforms, and common object variations. On the other hand, the features must also be sufficiently distinctive to identify specific objects among many alternatives. The difficulty of the object recognition problem is due in large part to the lack of success in finding such image features. However, recent research on the use of dense local features (e.g., Schmid & Mohr [19]) has shown that efficient recognition can often be achieved by using local image descriptors sampled at a large number of repeatable locations.

在杂乱的真实世界场景中进行目标识别，需要局部图像特征不被附近的杂乱或部分遮挡所影响。其特征至少要部分的对光照，3D投影变换，和常见的目标变化是不变的。另一方面，特征还要足够有区分性，以在很多备选中识别具体的目标。目标识别问题的困难是，找不到这样的特征。但是，最近的在使用密集局部特征方面的研究已经表明，高效的识别通常可以通过使用局部图像描述子来得到，这些描述子是在大量可重复的位置上采样得到的。

This paper presents a new method for image feature generation called the Scale Invariant Feature Transform (SIFT). This approach transforms an image into a large collection of local feature vectors, each of which is invariant to image translation, scaling, and rotation, and partially invariant to illumination changes and affine or 3D projection. Previous approaches to local feature generation lacked invariance to scale and were more sensitive to projective distortion and illumination change. The SIFT features share a number of properties in common with the responses of neurons in inferior temporal (IT) cortex in primate vision. This paper also describes improved approaches to indexing and model verification.

本文提出一种新的方法生成图像特征，称为尺度不变的特征变换(SIFT)。这种方法将一幅图像变换成大量局部特征向量的集合，每个特征都是对图像平移、缩放和旋转不变的，并部分的对光照变化和仿射或3D投影是不变的。之前的局部特征生成的方法缺少对尺度的不变性，对投影的变形和光照变化非常敏感。SIFT特征与灵长类视觉区的下颞皮质区中的神经元的响应有很多共同性质。本文还描述了索引和模型验证的改进方法。

The scale-invariant features are efficiently identified by using a staged filtering approach. The first stage identifies key locations in scale space by looking for locations that are maxima or minima of a difference-of-Gaussian function. Each point is used to generate a feature vector that describes the local image region sampled relative to its scale-space coordinate frame. The features achieve partial invariance to local variations, such as affine or 3D projections, by blurring image gradient locations. This approach is based on a model of the behavior of complex cells in the cerebral cortex of mammalian vision. The resulting feature vectors are called SIFT keys. In the current implementation, each image generates on the order of 1000 SIFT keys, a process that requires less than 1 second of computation time.

尺度不变特征可以使用一种分阶段滤波方法很高效的识别出来。第一阶段识别尺度空间中的关键位置，查找高斯差分函数的极大值或极小值的位置。每个点都用于生成一个特征向量，描述相对于其尺度空间坐标系采样的局部图像区域。这种特征获得了对其局部变化的部分不变性，比如仿射变换或3D投影，因为模糊了图像梯度位置。这种方法是基于哺乳动物视觉系统的大脑皮层的复杂细胞的行为模型的。得到的特征向量称为SIFT键。在目前的实现中，每幅图像生成1000个SIFT键的量级，这个过程耗时小于1秒。

The SIFT keys derived from an image are used in a nearest-neighbour approach to indexing to identify candidate object models. Collections of keys that agree on a potential model pose are first identified through a Hough transform hash table, and then through a least-squares fit to a final estimate of model parameters. When at least 3 keys agree on the model parameters with low residual, there is strong evidence for the presence of the object. Since there may be dozens of SIFT keys in the image of a typical object, it is possible to have substantial levels of occlusion in the image and yet retain high levels of reliability.

从一幅图像导出的SIFT键用于最近邻方法，通过索引以识别候选目标模型。在一个潜在的模型姿态上一致的键的集合，首先通过一个Hough变换hash表识别出来，然后通过一个最小二乘拟合得到最终的模型参数估计。当至少3个键在模型参数上一致，残差很低，那么就有很强的证据表明存在目标。由于在一幅图像中，一种典型目标可能有几十个SIFT键，所以即使有很大一部分遮挡，也有可能得到很高的可靠性。

The current object models are represented as 2D locations of SIFT keys that can undergo affine projection. Sufficient variation in feature location is allowed to recognize perspective projection of planar shapes at up to a 60 degree rotation away from the camera or to allow up to a 20 degree rotation of a 3D object.

目前的目标模型表示为2D位置的SIFT键，对仿射投影是不变的。特征位置的足够变化可以进行识别平面形状的视角投影，相对于相机最多有60度的旋转，或一个3D目标最多20度的旋转。

## 2. Related research

Object recognition is widely used in the machine vision industry for the purposes of inspection, registration, and manipulation. However, current commercial systems for object recognition depend almost exclusively on correlation-based template matching. While very effective for certain engineered environments, where object pose and illumination are tightly controlled, template matching becomes computationally infeasible when object rotation, scale, illumination, and 3D pose are allowed to vary, and even more so when dealing with partial visibility and large model databases.

目标识别广泛用于机器视觉工业，目标是检视，配准和操作。但是，目前的目标识别商用系统几乎完全依靠基于相关的模版匹配。虽然对于特定的精心设计的环境非常高效，其中目标姿态和光照是非常受控的，但当目标的旋转、尺度、光照和3D姿态可以变化时，模板匹配就变得计算上不可行了，当处理部分可见的目标和大型模型数据库时，就更加不可行了。

An alternative to searching all image locations for matches is to extract features from the image that are at least partially invariant to the image formation process and matching only to those features. Many candidate feature types have been proposed and explored, including line segments [6], groupings of edges [11, 14], and regions [2], among many other proposals. While these features have worked well for certain object classes, they are often not detected frequently enough or with sufficient stability to form a basis for reliable recognition.

除了搜索所有的图像位置进行匹配，另一种方法是从图像中提取特征，至少对图像形成的过程是部分不变的，只对这些特征进行匹配。提出并研究了很多候选特征类型，包括线段[6]，边缘分组[11,14]和区域[2]等等。这些特征对于特定的目标类别效果很好，但是有时候会检测不到，检测的稳定性不是很好，不能形成可靠的识别。

There has been recent work on developing much denser collections of image features. One approach has been to use a corner detector (more accurately, a detector of peaks in local image variation) to identify repeatable image locations, around which local image properties can be measured. Zhang et al. [23] used the Harris corner detector to identify feature locations for epipolar alignment of images taken from differing viewpoints. Rather than attempting to correlate regions from one image against all possible regions in a second image, large savings in computation time were achieved by only matching regions centered at corner points in each image.

最近有一些工作，开发更加密集的图像特征集合。一种方法是使用角点检测器（更准确的说，是检测图像局部变化的峰值），来识别重复的图像位置，在这些位置附近，可以度量局部图像特征。Zhang et al. [23]使用Harris角点检测器来对对极对齐的图像识别图像位置，这两幅图像是从不同的视角采集到的。这种方法不是将一幅图像中的区域与另一幅图像中其他所有可能的区域进行相关，而只对每幅图像中以角点为中心的区域进行匹配，所以可以节省很多时间。

For the object recognition problem, Schmid & Mohr [19] also used the Harris corner detector to identify interest points, and then created a local image descriptor at each interest point from an orientation-invariant vector of derivative-of-Gaussian image measurements. These image descriptors were used for robust object recognition by looking for multiple matching descriptors that satisfied object-based orientation and location constraints. This work was impressive both for the speed of recognition in a large database and the ability to handle cluttered images.

对目标识别问题，Schmid & Mohr [19]使用Harris角点检测器来识别感兴趣点，然后在每个感兴趣点上从一个高斯导数图像度量的方向不变的向量中创建了一个局部图像描述子。这个图像描述子通过查找多个匹配描述子，满足基于目标的方向和位置约束，用于进行稳健的目标识别。这个工作在大型数据库的识别速度，和处理杂乱图像的能力上，令人印象深刻。

The corner detectors used in these previous approaches have a major failing, which is that they examine an image at only a single scale. As the change in scale becomes significant, these detectors respond to different image points. Also, since the detector does not provide an indication of the object scale, it is necessary to create image descriptors and attempt matching at a large number of scales. This paper describes an efficient method to identify stable key locations in scale space. This means that different scalings of an image will have no effect on the set of key locations selected. Furthermore, an explicit scale is determined for each point, which allows the image description vector for that point to be sampled at an equivalent scale in each image. A canonical orientation is determined at each location, so that matching can be performed relative to a consistent local 2D coordinate frame. This allows for the use of more distinctive image descriptors than the rotation-invariant ones used by Schmid and Mohr, and the descriptor is further modified to improve its stability to changes in affine projection and illumination.

在这些之前的方法中的角点检测器，有一个主要的失败之处，即只在单个尺度上检查图像。在尺度变化明显时，这些检测器会对不同的图像点产生影响。同时，由于检测器并没有给出目标尺度的指示，需要创建图像描述子，并尝试在大量尺度上进行匹配。本文描述了一种高效的方法在尺度空间中识别稳定的关键位置。这个意思是，图像的不同缩放会对选择的关键位置没有任何影响。而且，在每个点上确定了一个显式的尺度，这允许对这个点的图像描述向量在每幅图像的等价尺度上进行采样。在每个位置上确定了一个典型方向，这样匹配可以在相对一致的局部2D坐标系中进行。这允许使用更加有区分性的图像描述子，而且这个描述进一步进行了修改，可以改进其对仿射投影和光照方面的稳定性。

Other approaches to appearance-based recognition include eigenspace matching [13], color histograms [20], and receptive field histograms [18]. These approaches have all been demonstrated successfully on isolated objects or pre-segmented images, but due to their more global features it has been difficult to extend them to cluttered and partially occluded images. Ohba & Ikeuchi [15] successfully apply the eigenspace approach to cluttered images by using many small local eigen-windows, but this then requires expensive search for each window in a new image, as with template matching.

其他的基于外观的识别方法包括，eigenspace匹配，色彩直方图，和感受野直方图。这些方法在孤立的物体中或预分割的图像中证明有效，但由于其更加全局的特征，很难将其拓展用到杂乱的和部分遮挡的图像。Ohba & Ikeuchi [15]成功的将eigenspace方法应用于杂乱的图像中，使用了很多小的局部eigen-窗口，但这需要昂贵的搜索每个窗口，这与模板匹配是类似的。

## 3. Key localization

We wish to identify locations in image scale space that are invariant with respect to image translation, scaling, and rotation, and are minimally affected by noise and small distortions. Lindeberg [8] has shown that under some rather general assumptions on scale invariance, the Gaussian kernel and its derivatives are the only possible smoothing kernels for scale space analysis.

我们希望识别图像尺度空间中的位置，对图像平移、缩放和旋转都是不变的，而且受到噪声和小的扰动的影响最小。Lindeberg表明，在一些很通用的尺度不变性假设下，高斯核及其导数是进行尺度空间分析的唯一可能的平滑核。

To achieve rotation invariance and a high level of efficiency, we have chosen to select key locations at maxima and minima of a difference of Gaussian function applied in scale space. This can be computed very efficiently by building an image pyramid with resampling between each level. Furthermore, it locates key points at regions and scales of high variation, making these locations particularly stable for characterizing the image. Crowley & Parker [4] and Lindeberg [9] have previously used the difference-of-Gaussian in scale space for other purposes. In the following, we describe a particularly efficient and stable method to detect and characterize the maxima and minima of this function.

为获得旋转不变性和高效率，我们选择的关键位置是，在尺度空间中的高斯函数差的极大极小值。这可以通过创建一个图像金字塔来进行高效的计算，在每个层级中进行重采样。而且，在区域和变化很大的尺度中定位关键点，使这些位置对于刻画图像特征尤其稳定。Crowley & Parker [4]和Lindeberg [9]之前在尺度空间中使用了高斯之差用于其他目的。下面，我们描述了一种尤其高效和稳定的方法来检测和刻画这个函数极大值和极小值的特征。

As the 2D Gaussian function is separable, its convolution with the input image can be efficiently computed by applying two passes of the 1D Gaussian function in the horizontal and vertical directions:

由于2D高斯函数是可分的，其与输入图像的卷积可以通过两次1D高斯函数的卷积来进行高效计算：

$$g(x) = \frac {1}{\sqrt{2π}σ} e^{-x^2/2σ^2}$$

For key localization, all smoothing operations are done using $σ=\sqrt 2$, which can be approximated with sufficient accuracy using 1D kernel with 7 sample points.

对于键定位，所有的平滑操作都是使用$σ=\sqrt 2$来完成的，这可以使用有7个采样点的1D核进行足够精度的近似。

The input image is first convolved with the Gaussian function using $σ=\sqrt 2$ to give an image A. This is then repeated a second time with a further incremental smoothing of $σ=\sqrt 2$ to give a new image, B, which now has an effective smoothing of σ = 2. The difference of Gaussian function is obtained by subtracting image B from A, resulting in a ratio of $2/\sqrt2 = \sqrt2$ between the two Gaussians.

输入图像首先用高斯核进行卷积，$σ=\sqrt 2$，得到图像A。然后重复进行一次，进一步进行增量平滑，$σ=\sqrt 2$，得到新图像B，这时的有效平滑参数为σ = 2。高斯函数的差分是通过从A中减去B，在两个高斯函数中得到比率$2/\sqrt2 = \sqrt2$。

To generate the next pyramid level, we resample the already smoothed image B using bilinear interpolation with a pixel spacing of 1.5 in each direction. While it may seem more natural to resample with a relative scale of $sqrt 2$, the only constraint is that sampling be frequent enough to detect peaks. The 1.5 spacing means that each new sample will be a constant linear combination of 4 adjacent pixels. This is efficient to compute and minimizes aliasing artifacts that would arise from changing the resampling coefficients.

为生成下一个金字塔层级，我们重新采样已经平滑的图像B，使用双线性插值，在每个方向上像素间距为1.5。用相对尺度$sqrt 2$来重新采样似乎更加自然，唯一的约束是，采样要足够频繁，以检测到峰值。1.5的间距意味着，每个新的样本都是4个临近像素的常数线性组合。这计算起来很高效，使锯齿伪影最小化，如果改变重采样的系数，就会出现更多的伪影。

Maxima and minima of this scale-space function are determined by comparing each pixel in the pyramid to its neighbours. First, a pixel is compared to its 8 neighbours at the same level of the pyramid. If it is a maxima or minima at this level, then the closest pixel location is calculated at the next lowest level of the pyramid, taking account of the 1.5 times resampling. If the pixel remains higher (or lower) than this closest pixel and its 8 neighbours, then the test is repeated for the level above. Since most pixels will be eliminated within a few comparisons, the cost of this detection is small and much lower than that of building the pyramid.

这个尺度空间函数的极大值和极小值，通过比较金字塔中的每个像素与其邻域来确定。首先，一个像素与其8邻域进行比较，在金字塔的同一层级。如果在这个层级是最大值或最小值，那么在下一个最低的金字塔层级计算最近的像素位置，考虑到1.5倍的重采样。如果像素仍然高于（或低于）临近的像素及其8邻域，那么这个测试到上一个层级继续进行。由于多数像素在几次比较之后就会被清除掉，这种检测的代价是很小的，比构建这个金字塔要小的多。

If the first level of the pyramid is sampled at the same rate as the input image, the highest spatial frequencies will be ignored. This is due to the initial smoothing, which is needed to provide separation of peaks for robust detection. Therefore, we expand the input image by a factor of 2, using bilinear interpolation, prior to building the pyramid. This gives on the order of 1000 key points for a typical 512x512 pixel image, compared to only a quarter as many without the initial expansion.

如果第一层级的金字塔与输入图像的采样率是一样的，那么就忽略掉最高的空间频率。这是因为初始平滑的原因，需要提供峰值的分离，以进行稳健的检测。因此，我们在构建金字塔之前将输入图像放大两倍，使用双线性插值。这对于典型的512x512的图像，可以有1000个关键点，如果没有初始放大的话，只会有1/4的数量。

### 3.1. SIFT key stability

To characterize the image at each key location, the smoothed image A at each level of the pyramid is processed to extract image gradients and orientations. At each pixel, $A_{ij}$, the image gradient magnitute, $M_{ij}$, and orientation, $R_{ij}$, are computed using pixel differences:

为在每个关键位置上刻画图像的特征，平滑的图像A，在金字塔的每个层级都进行处理，提取图像梯度和方向。在每个像素$A_{ij}$处，图像梯度幅度$M_{ij}$，和方向$R_{ij}$，都使用像素差值来进行计算：

$$M_{ij} = \sqrt {(A_{ij}-A_{i+1,j})^2 + (A_{ij}-A_{i,j+1})^2}$$

$$R_{ij} = atan2 (A_{ij}-A_{i+1,j}, A_{i,j+1}-A_{ij})$$

The pixel differences are efficient to compute and provide sufficient accuracy due to the substantial level of previous smoothing. The effective half-pixel shift in position is compensated for when determining key location.

像素差值计算起来很高效，由于之前的平滑，可以提供足够的准确率。在确定关键点位置时，位置上的有效半像素的偏移进行了补偿。

Robustness to illumination change is enhanced by thresholding the gradient magnitudes at a value of 0.1 times the maximum possible gradient value. This reduces the effect of a change in illumination direction for a surface with 3D relief, as an illumination change may result in large changes to gradient magnitude but is likely to have less influence on gradient orientation.

对光照变化的稳健性，通过对梯度幅值进行阈值过滤得到增强，阈值为最大可能的梯度值的0.1倍。这降低了光照方向的变化的影响，因为光照的变化会导致梯度幅度的很大变化，但梯度的方向受到的影响会更小。

Each key location is assigned a canonical orientation so that the image descriptors are invariant to rotation. In order to make this as stable as possible against lighting or contrast changes, the orientation is determined by the peak in a histogram of local image gradient orientations. The orientation histogram is created using a Gaussian-weighted window with σ of 3 times that of the current smoothing scale. These weights are multiplied by the thresholded gradient values and accumulated in the histogram at locations corresponding to the orientation, R_{ij}. The histogram has 36 bins covering the 360 degree range of rotations, and is smoothed prior to the peak selection.

每个关键位置都指定了一个典型的方向，这样图像描述子就对旋转是不变的。为使这个尽可能的对光照或对比度变化稳定，其方向由局部图像梯度方向的直方图峰值来确定。方向直方图，是用高斯加权的窗创建的，σ为当前平滑尺度的3倍。这种加权乘以阈值筛选过后的梯度值，并在直方图中对应方向R_{ij}的位置累加。直方图有36个bins，覆盖了360度方向的旋转，在峰值选择之前进行平滑。

The stability of the resulting keys can be tested by subjecting natural images to affine projection, contrast and brightness changes, and addition of noise. The location of each key detected in the first image can be predicted in the transformed image from knowledge of the transform parameters. This framework was used to select the various sampling and smoothing parameters given above, so that maximum efficiency could be obtained while retaining stability to changes.

得到的关键点的稳定性，可以将自然图像经过仿射投影，对比度和亮度变化，和增加噪声以进行测试。第一幅图像中检测到的每个关键点的位置，在变换后的图像中，可以通过变换参数来进行预测得到。这个框架用于选择各种不同的上面给定的采样和平滑参数，这样可以得到最大的效率，同时保持变化的稳定性。

Figure 1 shows a relatively small number of keys detected over a 2 octave range of only the larger scales (to avoid excessive clutter). Each key is shown as a square, with a line from the center to one side of the square indicating orientation. In the second half of this figure, the image is rotated by 15 degrees, scaled by a factor of 0.9, and stretched by a factor of 1.1 in the horizontal direction. The pixel intensities, in the range of 0 to 1, have 0.1 subtracted from their brightness values and the contrast reduced by multiplication by 0.9. Random pixel noise is then added to give less than 5 bits/pixel of signal. In spite of these transformations, 78% of the keys in the first image had closely matching keys in the second image at the predicted locations, scales, and orientations.

图1展示了只在较大尺度上的两个尺度范围内相对较小数量的关键点（以防止过多的杂乱）。每个关键点展示为一个正方形，带有一个从中心出来的线段，指示方向。在图像的第二半，图像旋转了15度，缩放系数为0.9，水平方向拉伸系数为1.1。像素灰度的范围为0到1，亮度值减去了0.1，对比度乘以了0.9。加入了随机像素噪声。即使有这些变换，第一幅图像中78%的关键点，在第二幅图像中的预测位置、尺度和方向上都有紧密的匹配关键点。

The overall stability of the keys to image transformations can be judged from Table 2. Each entry in this table is generated from combining the results of 20 diverse test images and summarizes the matching of about 15,000 keys. Each line of the table shows a particular image transformation. The first figure gives the percent of keys that have a matching key in the transformed image within σ in location (relative to scale for that key) and a factor of 1.5 in scale. The second column gives the percent that match these criteria as well as having an orientation within 20 degrees of the prediction.

关键点对图像变换的总体稳定性，可以从表2中看出。表中的每个条目都是通过将20个各种各样的测试图像结合到一起生成的，并总结了大约15000个关键点的匹配程度。这个表格的每一行都代表了一个特定的图像变换。第一个数给出了，在变换的图像中有匹配的关键点的百分比数量。第二列给出了匹配这些规则的百分比，同时预测的方向在20度以内。

## 4. Local image description

Given a stable location, scale, and orientation for each key, it is now possible to describe the local image region in a manner invariant to these transformations. In addition, it is desirable to make this representation robust against small shifts in local geometry, such as arise from affine or 3D projection. One approach to this is suggested by the response properties of complex neurons in the visual cortex, in which a feature position is allowed to vary over a small region while orientation and spatial frequency specificity are maintained. Edelman, Intrator & Poggio [5] have performed experiments that simulated the responses of complex neurons to different 3D views of computer graphic models, and found that the complex cell outputs provided much better discrimination than simple correlation-based matching. This can be seen, for example, if an affine projection stretches an image in one direction relative to another, which changes the relative locations of gradient features while having a smaller effect on their orientations and spatial frequencies.

对每个关键点给定一个稳定的位置，尺度和方向，现在可能以一种对这些变换不变的方式，来描述这个局部图像区域。另外，让这种表示对局部几何的很小变化稳健，比如仿射投影或3D投影，这是很理想的。一种方法可以通过观察视觉皮层的复杂神经元的响应性质来得到，其中特征位置可以在一个小范围内变化，同时方向和空间频率都得到保持。Edelman, Intrator & Poggio [5]进行了试验，模拟了复杂神经元对计算机图形学模型的不同的3D视角的响应，发现复杂的细胞的输出给出了比简单的基于相关的匹配的好的多的区分能力。这可以看作是，比如，如果一个仿射投影在一个方向上拉伸了图像，这改变了梯度特征的相对位置，但对其方向和空间频率的作用要小的多。

This robustness to local geometric distortion can be obtained by representing the local image region with multiple images representing each of a number of orientations (referred to as orientation planes). Each orientation plane contains only the gradients corresponding to that orientation, with linear interpolation used for intermediate orientations. Each orientation plane is blurred and resampled to allow for larger shifts in positions of the gradients.

这种对局部几何形变的稳健性的获得，可以通过用多个图像表示局部图像区域，每个表示一定数量的方向（称为方向平面）。每个方向平面包含的梯度，对应着那个方向，用线性差值作为中间方向。每个方向平面都进行了模糊和重采样，允许梯度位置的更大偏移。

This approach can be efficiently implemented by using the same precomputed gradients and orientations for each level of the pyramid that were used for orientation selection. For each keypoint, we use the pixel sampling from the pyramid level at which the key was detected. The pixels that fall in a circle of radius 8 pixels around the key location are inserted into the orientation planes. The orientation is measured relative to that of the key by subtracting the key’s orientation. For our experiments we used 8 orientation planes, each sampled over a 4x4 grid of locations, with a sample spacing 4 times that of the pixel spacing used for gradient detection. The blurring is achieved by allocating the gradient of each pixel among its 8 closest neighbors in the sample grid, using linear interpolation in orientation and the two spatial dimensions. This implementation is much more efficient than performing explicit blurring and resampling, yet gives almost equivalent results.

这种方法可以用预先计算好的各个层级的金字塔的梯度和方向，进行高效的实现，这些梯度和方向是用于方向选择的。对于每个关键点，我们使用的像素采样是从检测到这个关键的金字塔层级的。关键位置点的像素周围半径8个像素的圆内的点，插入到方向平面中。方向的度量是相对于关键点的，方式是减去关键点的方向。对于我们的试验，我们使用8个方向平面，每个都在4x4的位置网格上进行采样，样本间距4倍于用于梯度检测的像素间距。模糊效果的获得，是通过将每个像素的梯度，置于其最近的8个邻域中的采样网格中，使在方向和两个空间维度上使用的是线性插值。这种实现比进行显式的模糊和重采样高效的多，而且得到的是类似的效果。

In order to sample the image at a larger scale, the same process is repeated for a second level of the pyramid one octave higher. However, this time a 2x2 rather than 4x4 sample region is used. This means that approximately the same image region will be examined at both scales, so that any nearby occlusions will not affect one scale more than the other. Therefore, the total number of samples in the SIFT key vector, from both scales, is 8x4x4+8x2x2 or 160 elements, giving enough measurements for high specificity.

为在更大的尺度上对图像进行采样，对于金字塔的另一个层次，重复同样的过程。但是，这次使用的是2x2的区域，而不是4x4的区域。这意味着，同样的区域会在不同的尺度进行检验，所以任何附近的遮挡都会影响两个尺度。因此，在两个尺度上的SIFT关键点向量的样本总数，是8x4x4+8x2x2或160个元素，对于很高的具体性给出了足够的度量。

## 5. Indexing and matching

For indexing, we need to store the SIFT keys for sample images and then identify matching keys from new images. The problem of identifying the most similar keys for high dimensional vectors is known to have high complexity if an exact solution is required. However, a modification of the k-d tree algorithm called the best-bin-first search method (Beis & Lowe [3]) can identify the nearest neighbors with high probability using only a limited amount of computation. To further improve the efficiency of the best-bin-first algorithm, the SIFT key samples generated at the larger scale are given twice the weight of those at the smaller scale. This means that the larger scale is in effect able to filter the most likely neighbours for checking at the smaller scale. This also improves recognition performance by giving more weight to the least-noisy scale. In our experiments, it is possible to have a cut-off for examining at most 200 neighbors in a probabilistic best-bin-first search of 30,000 key vectors with almost no loss of performance compared to finding an exact solution.

对于索引，我们需要对样本图像存储SIFT关键点，并从新图像中识别匹配的关键点。对于高维向量，识别最相似的点的问题，如果需要精确的解，其复杂度是很高的。但是，一种k-d树算法的修正，称为best-bin-first搜索方法，可以在很短时间内以很高的概率识别最近邻。为进一步改进best-bin-first算法的效率，在更大尺度上生成的SIFT关键点样本，所给出的权重，是更小尺度上的两倍。其意思是，更大的尺度实际上可以滤出最可能的邻域的，以在更小的尺度上进行检查。这也改进了识别性能，给噪声最少的尺度上更多的权重。在我们的试验中，在best-bin-search 30000个关键点向量时，可以只检查最多200个邻域，与找到精确解相比，几乎不损失性能。

An efficient way to cluster reliable model hypotheses is to use the Hough transform [1] to search for keys that agree upon a particular model pose. Each model key in the database contains a record of the key’s parameters relative to the model coordinate system. Therefore, we can create an entry in a hash table predicting the model location, orientation, and scale from the match hypothesis. We use a bin size of 30 degrees for orientation, a factor of 2 for scale, and 0.25 times the maximum model dimension for location. These rather broad bin sizes allow for clustering even in the presence of substantial geometric distortion, such as due to a change in 3D viewpoint. To avoid the problem of boundary effects in hashing, each hypothesis is hashed into the 2 closest bins in each dimension, giving a total of 16 hash table entries for each hypothesis.

一种高效的聚类可靠的模型假设的方法是，使用Hough变换来搜索对特定模型姿态一致的关键点。数据库中的每个模型关键点，都包含这个关键点的相对于模型坐标系的参数的记录。因此，我们可以创建一个hash表中的条目，从模型假设中预测模型的位置，方向和尺度。我们使用的bin大小为方向30度，尺度因子为2，模型维度最大值的0.25倍用于方向。这些相对较大的bin大小，允许在几何形变较大的存在时的聚类，比如3D视角的改变。为防止hash时的边界效应，每个假设都hash到每个维度中2个最近的bins，对每个假设都给出总计16个hash表条目。

## 6. Solution for affine parameters

The hash table is searched to identify all clusters of at least 3 entries in a bin, and the bins are sorted into decreasing order of size. Each such cluster is then subject to a verification procedure in which a least-squares solution is performed for the affine projection parameters relating the model to the image.

搜索hash表以识别在一个bin里有至少三个条目的所有簇，这些簇以大小的降序来排序。每个这样的簇然后进行一个验证过程，对仿射投影参数进行一个最小二乘求解，这些参数将模型与图像联系到一起。

The affine transformation of a model point $[x,y]^T$ to an image point $[u,v]^T$ can be written as 一个模型点$[x,y]^T$仿射变换到一个图像点$[u,v]^T$可以写成

$$\left[ \begin{matrix}  u \\ v \end{matrix} \right] = \left[ \begin{matrix}  m_1 & m_2 \\ m_3 & m_4 \end{matrix} \right] \left[ \begin{matrix}  x \\ y \end{matrix} \right] + \left[ \begin{matrix}  t_x \\ t_y \end{matrix} \right]$$

where the model translation is $[t_x, t_y]^T$ and the affine rotation, scale, and stretch are represented by the $m_i$ parameters. 其中模型平移是$[t_x, t_y]^T$，仿射旋转、缩放和拉伸由参数$m_i$表示。

We wish to solve for the transformation parameters, so the equation above can be rewritten as 我们希望求解变换参数，所以上述等式可以重写成

$$\left[ \begin{matrix}  x & y & 0 & 0 & 1 & 0 \\ 0 & 0 & x & y & 0 & 1 \\ ... \\ ... \end{matrix} \right] \left[ \begin{matrix}  m_1 \\ m_2 \\ m_3 \\ m_4 \\ t_x \\ t_y \end{matrix} \right] = \left[ \begin{matrix}  u \\ v \\ . \\ . \\. \end{matrix} \right]$$

This equation shows a single match, but any number of further matches can be added, with each match contributing two more rows to the first and last matrix. At least 3 matches are needed to provide a solution.

上式给出了一个单匹配，但任意数量的更多匹配都可以加上去，每个匹配贡献出新的两行。至少需要三个匹配才能给出一个解答。

We can write this linear system as 我们将这个线性系统可以写成

Ax=b

The least-squares solution for the parameters x can be determined by solving the corresponding normal equations, 参数x的最小二乘解可以通过求解对应的normal方程确定

$$x = [A^T A]^{-1} A^T b$$

which minimizes the sum of the squares of the distances from the projected model locations to the corresponding image locations. This least-squares approach could readily be extended to solving for 3D pose and internal parameters of articulated and flexible objects [12].

方程对投影模型位置到对应的图像位置的距离的平方和进行了最小化。这种最小二乘方法可以很容易的拓展，以求解3D姿态和铰接和灵活目标的内部参数。

Outliers can now be removed by checking for agreement between each image feature and the model, given the parameter solution. Each match must agree within 15 degrees orientation, $\sqrt 2$ change in scale, and 0.2 times maximum model size in terms of location. If fewer than 3 points remain after discarding outliers, then the match is rejected. If any outliers are discarded, the least-squares solution is re-solved with the remaining points.

给定参数解，离群点可以通过检查每幅图像和模型的一致性进行去除。每个匹配的一致性是在15度方向范围内，尺度变化$\sqrt 2$内，位置在最大模型大小的0.2倍以内。如果丢弃离群点后，只剩下不到3个点，那么这个匹配就被拒绝了。如果抛弃了任意的离群点，必须用剩余的点重新求解最小二乘解。

## 7. Experiments

The affine solution provides a good approximation to perspective projection of planar objects, so planar models provide a good initial test of the approach. The top row of Figure 3 shows three model images of rectangular planar faces of objects. The figure also shows a cluttered image containing the planar objects, and the same image is shown over-layed with the models following recognition. The model keys that are displayed are the ones used for recognition and final least-squares solution. Since only 3 keys are needed for robust recognition, it can be seen that the solutions are highly redundant and would survive substantial occlusion. Also shown are the rectangular borders of the model images, projected using the affine transform from the least-square solution. These closely agree with the true borders of the planar regions in the image, except for small errors introduced by the perspective projection. Similar experiments have been performed for many images of planar objects, and the recognition has proven to be robust to at least a 60 degree rotation of the object in any direction away from the camera.

仿射解是平面目标的透视投影的很好近似，所以平面模型是这种方法的一个很好的初始测试。图3的最顶行是三幅模型图像，是目标的矩形平面表示。图中还有包含平面目标的杂乱图像，还有叠加了识别模型的图像。展示出来的模型关键点，是用于识别和最终的最小二乘解的。由于只需要三个关键点就可以进行很稳健的识别，可以发现，这些解是高度冗余的，可以容忍很高的遮挡。同时展示的，还有模型图像的矩形边界，从最小二乘解中用仿射变换进行了投影。这与图像中平面目标的真实边界是高度吻合的，除了有一些由于透视投影导致的小误差。对很多平面目标的图像作了很多类似的试验，这个识别已经证明至少对60度以内的旋转都是稳健的。

Although the model images and affine parameters do not account for rotation in depth of 3D objects, they are still sufficient to perform robust recognition of 3D objects over about a 20 degree range of rotation in depth away from each model view. An example of three model images is shown in the top row of Figure 4. The models were photographed on a black background, and object outlines extracted by segmenting out the background region. An example of recognition is shown in the same figure, again showing the SIFT keys used for recognition. The object outlines are projected using the affine parameter solution, but this time the agreement is not as close because the solution does not account for rotation in depth. Figure 5 shows more examples in which there is significant partial occlusion.

虽然模型图像和仿射参数没有考虑3D目标的旋转，但对于3D目标在20度以内的旋转，还是足以进行很稳健的识别的。图4的最上面一行展示了三个模型图像。模型是在黑色背景中拍摄的，目标的轮廓是通过从背景区域中分割来提取的。识别的例子如图4所示，再一次展示了用于识别的SIFT关键点。目标轮廓是使用仿射参数解来投影的，但这时的一致性并不接近，因为这个解并没有考虑到深度上的旋转。图5给出来更多的例子，其中包含显著的部分遮挡。

The images in these examples are of size 384x512 pixels. The computation times for recognition of all objects in each image are about 1.5 seconds on a Sun Sparc 10 processor, with about 0.9 seconds required to build the scale-space pyramid and identify the SIFT keys, and about 0.6 seconds to perform indexing and least-squares verification. This does not include time to pre-process each model image, which would be about 1 second per image, but would only need to be done once for initial entry into a model database.

这些试验中的图像大小都是384x512像素的。每幅图像中的所有目标，识别的计算时间，在一台Sun Sparc 10处理器上大约为1.5秒，大约需要0.9秒来构建尺度空间金字塔，以及识别SIFT关键点，0.6秒进行索引和最小二乘验证。这并没有包括预处理每幅模型图像的时间，大约是每幅图像1秒，但这只需要处理一次，以形成模型数据库的初始条目。

The illumination invariance of the SIFT keys is demonstrated in Figure 6. The two images are of the same scene from the same viewpoint, except that the first image is illuminated from the upper left and the second from the center right. The full recognition system is run to identify the second image using the first image as the model, and the second image is correctly recognized as matching the first. Only SIFT keys that were part of the recognition are shown. There were 273 keys that were verified as part of the final match, which means that in each case not only was the same key detected at the same location, but it also was the closest match to the correct corresponding key in the second image. Any 3 of these keys would be sufficient for recognition. While matching keys are not found in some regions where highlights or shadows change (for example on the shiny top of the camera) in general the keys show good invariance to illumination change.

SIFT关键点的光照不变性如图6所示。两幅图像是同一个场景，同一个视角，但第一幅图像的光照是左上，第二幅图像是右中。用第一幅图像作为模型，对第二幅图像进行识别，我们运行了完整的识别系统，结果是第二幅图像得到了正确的识别与匹配。识别用到的一部分SIFT关键点得到了展示。共有273个关键点被验证为最终匹配的一部分，这意味着在每种情况中，在相同的位置上不仅检测到的同样的关键点，而且在第二幅图像中这是最接近的与对应的正确的关键点的匹配。这些关键点的任意三个足以进行识别。在某些高光或阴影变化区域，有些匹配关键点没有找到（比如在相机很亮的顶部），一般来说，关键点对于光照变化有很好的不变性。

## 8. Connections to biological vision

The performance of human vision is obviously far superior to that of current computer vision systems, so there is potentially much to be gained by emulating biological processes. Fortunately, there have been dramatic improvements within the past few years in understanding how object recognition is accomplished in animals and humans.

人类视觉的性能明显比目前的计算机视觉系统要优秀，所以仿真生理过程会有很多收获。幸运的是，在过去几年中，对于动物和人类怎样进行目标识别的过程，有很大的进展。

Recent research in neuroscience has shown that object recognition in primates makes use of features of intermediate complexity that are largely invariant to changes in scale, location, and illumination (Tanaka [21], Perrett & Oram [16]). Some examples of such intermediate features found in inferior temporal cortex (IT) are neurons that respond to a dark five-sided star shape, a circle with a thin protruding element, or a horizontal textured region within a triangular boundary. These neurons maintain highly specific responses to shape features that appear anywhere within a large portion of the visual field and over a several octave range of scales (Ito et. al [7]). The complexity of many of these features appears to be roughly the same as for the current SIFT features, although there are also some neurons that respond to more complex shapes, such as faces. Many of the neurons respond to color and texture properties in addition to shape. The feature responses have been shown to depend on previous visual learning from exposure to specific objects containing the features (Logothetis, Pauls & Poggio [10]). These features appear to be derived in the brain by a highly computation-intensive parallel process, which is quite different from the staged filtering approach given in this paper. However, the results are much the same: an image is transformed into a large set of local features that each match a small fraction of potential objects yet are largely invariant to common viewing transformations.

最近在神经科学上的研究表明，灵长类动物的目标识别，会利用中间层复杂的特征，对尺度、位置和光照都大致是不变的。这种中间特征的一些例子包括，下颞皮质层是一些神经元，对暗五角星、带有突出元素的圆形，或带有三角形边缘的水平纹理区域有所响应。这些神经元对于在视野中任意位置以及几种尺度上的形状特征，有非常独特的反应。很多这种特征的复杂度似乎与目前的SIFT特征大致一样，虽然也有一些神经元对更加复杂的形状有响应，如人脸。很多神经元除了对形状，对于色彩和纹理性质也有反应。特征响应依赖于之前的视觉学习，即暴露在包含这些特征的具体目标中的学习。这些特征似乎是从大脑中推断出来的，通过计算量很大的高度并行的过程，与本文给出的分阶段的滤波的过程不太相似。但是，结果是大致一样的：一幅图像变换成局部特征的合集，每个与一些可能的目标匹配，这些对于常见的视角变换是大致不变的。

It is also known that object recognition in the brain depends on a serial process of attention to bind features to object interpretations, determine pose, and segment an object from a cluttered background [22]. This process is presumably playing the same role in verification as the parameter solving and outlier detection used in this paper, since the accuracy of interpretations can often depend on enforcing a single viewpoint constraint [11].

大家也知道，大脑中的目标识别过程，依赖于一系列注意力过程，将特征进行目标解释，确定姿态，从杂乱的背景中分割出一个目标。这个过程在验证中也起了同样的作用，就像本文中使用的参数求解和离群点检测一样，因为解释的准确率通常依赖于施加一个视角约束。

## 9. Conclusions and comments

The SIFT features improve on previous approaches by being largely invariant to changes in scale, illumination, and local affine distortions. The large number of features in a typical image allow for robust recognition under partial occlusion in cluttered images. A final stage that solves for affine model parameters allows for more accurate verification and pose determination than in approaches that rely only on indexing.

在之前方法的基础上改进的SIFT特征，对尺度、光照和局部仿射形变大致都是不变的。一幅典型的图像中通常有大量的特征，这可以在杂乱图像中的部分遮挡时都可以进行稳健的识别。最后的求解仿射模型参数的阶段，可以进行精确的验证和确定姿态。

An important area for further research is to build models from multiple views that represent the 3D structure of objects. This would have the further advantage that keys from multiple viewing conditions could be combined into a single model, thereby increasing the probability of finding matches in new views. The models could be true 3D representations based on structure-from-motion solutions, or could represent the space of appearance in terms of automated clustering and interpolation (Pope & Lowe [17]). An advantage of the latter approach is that it could also model non-rigid deformations.

未来研究的一个重要领域是，从多视角构建模型，表示目标的3D结构。这可能会有进一步的优势，即多视角条件下的关键点可以结合到单个模型中，因此增加了在新视角中找到匹配的概率。这些模型可以是真3D表示，基于structure-from-motion的解，也可以用自动聚类和插值来表示外观空间。后者方法的一个优势是，可以对非刚体形变进行建模。

The recognition performance could be further improved by adding new SIFT feature types to incorporate color, texture, and edge groupings, as well as varying feature sizes and offsets. Scale-invariant edge groupings that make local figure-ground discriminations would be particularly useful at object boundaries where background clutter can interfere with other features. The indexing and verification framework allows for all types of scale and rotation invariant features to be incorporated into a single model representation. Maximum robustness would be achieved by detecting many different feature types and relying on the indexing and clustering to select those that are most useful in a particular image.

识别性能可以通过增加新的SIFT特征类型以进行改进，包括色彩、纹理和边缘分组，还包括变化特征大小和偏移。尺度不变的边缘分组，可以进行局部的图像-背景区分，在目标边缘处会非常有用，其中背景杂乱会与其他特征进行干扰。索引和验证框架，使得可以将各种尺度和旋转不变的特征都纳入到单个模型表示中。通过检测多个不同的特征类型，依靠索引和聚类来选择在特征图像中最有用的，可以达到最大的稳健性。