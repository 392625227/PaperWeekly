# Deep learning

Yann LeCun, Yoshua Bengio & Geoffrey Hinton

Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.

深度学习使得由多个处理层组成的计算模型，可以学习数据的表示，形成多层次的抽象。这些方法已经急剧的改进了语音识别，视觉目标识别，目标检测和很多其他领域，如药物发现和基因组学的目前最好状态。深度学习通过使用反向传播算法，来指示机器应当怎样改变其内部参数，用于从上一层的表示中计算每层的表示，在大型数据集中发现了错综复杂的结构。深度卷积网络带来了在处理图像、视频，语音和音频方面的突破，而循环网络在序列数据的处理中大放光芒，如文本和语音。

## 0. Introduction

Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning.

机器学习技术为现代社会的很多方面提供了赋能：从网络搜索，到社交网络中的内容过滤，到电子商务网站中的推荐，在消费者产品，如相机和智能手机中，也越来越多的存在。机器学习系统用于识别图像的目标，将语音转录成文本，根据用户的兴趣匹配新的项目，帖子或产品，选择搜索到的相关结果。越来越多的是，这些应用都使用一类技术称为深度学习。

Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input.

传统的机器学习技术，在处理原始形式的自然数据上，能力有限。几十年以来，构建一个模式识别或机器学习系统，需要仔细的设计，或很多的领域专业知识，来设计一个特征提取器，将原始数据（比如一幅图像的像素值），变换到合适的内部表示或特征向量，然后学习系统，通常是一个分类器，可以从输入中对模式进行检测或分类。

Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.

表示学习是一类方法，其输入是原始数据，自动发现用于检测或分类的表示信息。深度学习方法，是表示学习方法，有多个层次的表示，网络由简单，但非线性的模块组成，每个模块将表示从一个层级转换到另一个更抽象的层级。这种变换如果足够多，就可以学到非常复杂的函数。对于分类任务，更高层的表示将对于区分更重要的方面进行了放大，并抑制了相关的变化。例如，一幅图像，形式为像素值的阵列，第一层学习到的特征典型的是，包含或不包含特定方向或位置的图像。第二层一般检测图案，对特定排布的边缘进行定位，而与边缘位置的小的变化无关。第三层可能将一些图案组合成更大，对应一些熟悉的目标的特定部分，后续的层会检测到的这些部分的组合的目标。深度学习的关键部分是，这些层的特征并不是由人设计的：它们是用通用目标学习过程学到的。

Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation.

深度学习在解决问题中取得了很大进展，非常擅长于在高维数据中发现复杂的结构，在很多科学、商业和政府的领域都可以应用。除了在图像识别和语音识别中取得了新的记录，在预测可能的药物分子，分析粒子加速数据，重建脑回路，在基因表现和疾病的非编码DNA变异效果预测中，也击败了其他机器学习技术。更令人惊讶的是，深度学习在自然语言处理的很多任务中，得到了非常有希望的结果，特别是话题分类，情绪分析，问题回答和语言翻译。

We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.

我们认为，深度学习在不远的将来会有更多的成功，而且需要非常少的手工工程工作，所以可以很容易的利用现在越来越多的计算能力和数据。新提出的深度神经网络的学习算法和架构，只会加速这个过程。

## 1. Supervised learning

The most common form of machine learning, deep or not, is supervised learning. Imagine that we want to build a system that can classify images as containing, say, a house, a car, a person or a pet. We first collect a large data set of images of houses, cars, people and pets, each labelled with its category. During training, the machine is shown an image and produces an output in the form of a vector of scores, one for each category. We want the desired category to have the highest score of all categories, but this is unlikely to happen before training. We compute an objective function that measures the error (or distance) between the output scores and the desired pattern of scores. The machine then modifies its internal adjustable parameters to reduce this error. These adjustable parameters, often called weights, are real numbers that can be seen as ‘knobs’ that define the input–output function of the machine. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights, and hundreds of millions of labelled examples with which to train the machine.

无论是否是深度学习，机器学习的最常见形式，是监督学习。假设我们想要构建一个系统，将图像分类为，比如，包含一个房子，一辆车，一个人或一个宠物。我们首先收集一个大型图像数据集，包含房子，车辆，人和宠物，每幅图像都标记好其类别。在训练的过程中，机器以一幅图像为输入，生成一个输出，其形式是一个向量的分数，每个类别一个分数。我们希望期望的类别在所有类别中有最高的分数，但这在训练之前不太可能发生。我们计算一个目标函数，度量输出分数和期望的分数模式之间的误差（或距离）。网络会调整其内部参数，以降低此误差。这些可调整的参数，通常称之为权重，是一些实数，可以视为一些把手，定义了网络的输入输出函数。在一个典型的深度学习系统中，可能会有数千万上亿个这些可调整的参数，会使用数百万上千万个标注的样本来训练这个网络。

To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. The weight vector is then adjusted in the opposite direction to the gradient vector.

为合理的调整这些权重向量，学习算法会计算一个梯度向量，对每个权重，会有一个指示，如果权重增加了一个小量，那么会降低或增加多少误差。权重向量然后会在梯度向量相反的方向进行调整。

The objective function, averaged over all the training examples, can be seen as a kind of hilly landscape in the high-dimensional space of weight values. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average.

目标函数，在所有训练样本上进行平均，可以视为高维权重空间的曲面。负梯度向量指出了这个曲面的最速下降方向，带到离最小值更近的地方，其输出误差平均会更低。

In practice, most practitioners use a procedure called stochastic gradient descent (SGD). This consists of showing the input vector for a few examples, computing the outputs and the errors, computing the average gradient for those examples, and adjusting the weights accordingly. The process is repeated for many small sets of examples from the training set until the average of the objective function stops decreasing. It is called stochastic because each small set of examples gives a noisy estimate of the average gradient over all examples. This simple procedure usually finds a good set of weights surprisingly quickly when compared with far more elaborate optimization techniques. After training, the performance of the system is measured  on a different set of examples called a test set. This serves to test the generalization ability of the machine — its ability to produce sensible answers on new inputs that it has never seen during training.

实践中，多数人都会使用SGD的过程。这包括，对几个样本计算其输入向量，计算输出及其误差，计算这些样本的平均梯度，并相应的调整权重。这个过程对很多小的样本集进行重复，直到目标函数停止下降。这称为随机的，因为每个小的样本集会给出所有样本的平均梯度的含噪估计。与一些复杂的多的优化技术相比，这个简单的过程通常会很快的找到权重集合。在训练之后，系统的性能会在一个不同的样本集合中进行度量，称为测试集。其用处是测试网络的泛化能力，也就是在新的输入中产生有意义的结果，这些输入是在训练中未曾见过的。

Many of the current practical applications of machine learning use linear classifiers on top of hand-engineered features. A two-class linear classifier computes a weighted sum of the feature vector components. If the weighted sum is above a threshold, the input is classified as belonging to a particular category.

很多目前的机器学习的实际应用都在手工设计的特征上使用线性分类器。一个两类线性分类器会计算特征向量的加权和。如果加权和超过一个阈值，输入就被分类为，属于一个特定的类别。

Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane. But problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech, while being very sensitive to particular minute variations (for example, the difference between a white wolf and a breed of wolf-like white dog called a Samoyed). At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be very similar to each other. A linear classifier, or any other ‘shallow’ classifier operating on raw pixels could not possibly distinguish the latter two, while putting the former two in the same category. This is why shallow classifiers require a good feature extractor that solves the selectivity–invariance dilemma — one that produces representations that are selective to the aspects of the image that are important for discrimination, but that are invariant to irrelevant aspects such as the pose of the animal. To make classifiers more powerful, one can use generic non-linear features, as with kernel methods, but generic features such as those arising with the Gaussian kernel do not allow the learner to generalize well far from the training examples. The conventional option is to hand design good feature extractors, which requires a considerable amount of engineering skill and domain expertise. But this can all be avoided if good features can be learned automatically using a general-purpose learning procedure. This is the key advantage of deep learning.

自从1960s以后，我们知道，线性分类器智能将输入空间分成很简单的区域，即由超平面分隔的半空间。但像图像和语音识别这样的问题，需要输入-输出函数对输入中无关的变化要不敏感，比如目标位置、方向或光照的变化，或语音的音高或方言，但对特定的小的变化要敏感（如，白狼和与狼很类似的白狗Samoyed的区别）。在像素层次，在不同环境中的两只不同姿态的Samoyed的图像可能会非常不同，而相同位置，类似背景的一只Samoyed和一只狼的两幅图像，则会非常相似。一个线性分类器，或任何其他浅层的分类器，在原始像素上运算的话，不可能区分后面的两者，而会将前者的两幅图像归为同一类。这就是为什么浅层的分类器需要一个好的特征提取器，来解决敏感-不变性的两难问题，要产生的表示，会对图像的不同部分产生有选择的表示，对有区分性的重要，但对无关的方面则不变，如动物的姿态。为使分类器更加强力，可以使用通用的非线性特征，如用核方法，但用高斯核生成的这样的通用特征，对于训练样本距离很远的样本泛化性能会不好。传统的选项是，手工设计好的特征选择器，这需要很多工程能力和领域的专业知识。但如果好的特征可以自动学习得到，使用通用目标的学习过程，那么这就可以避免了。这是深度学习的关键优势。

A deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning, and many of which compute non-linear input–output mappings. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations such as the background, pose, lighting and surrounding objects.

一个深度学习架构是单层模块的多层堆叠，全部（或多数）都是与学习相关的，很多计算的都是非线性的输入-输出映射。堆叠中的每个模块都对其输入进行变换，以增加表示的选择性和不变性。在多个非线性层下，比如深度在5至20之间，系统可以实现输入的极其复杂的函数，对一些小的细节敏感（区分Samoyed和白狼），对大的不相关的变化不敏感（如背景，姿态，光照和周围的目标）。

## 2. Backpropagation to train multilayer architectures

From the earliest days of pattern recognition, the aim of researchers has been to replace hand-engineered features with trainable multilayer networks, but despite its simplicity, the solution was not widely understood until the mid 1980s. As it turns out, multilayer architectures can be trained by simple stochastic gradient descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s.

从模式识别的早期，研究者的目的就是将手工设计的特征替换成可训练的多层网络，但尽管很简单，这个解决方案直到1980s才得到广为理解。结果是，多层架构可以通过简单的SGD进行训练。只要这些模块是输入及其内部权重的相对平滑的函数，人就可以使用反向传播过程计算梯度。在1970s和1980s，几个不同的小组独立发现了这个是可以做的，而且这个会好用的。

The backpropagation procedure to compute the gradient of an objective function with respect to the weights of a multilayer stack of modules is nothing more than a practical application of the chain rule for derivatives. The key insight is that the derivative (or gradient) of the objective with respect to the input of a module can be computed by working backwards from the gradient with respect to the output of that module (or the input of the subsequent module) (Fig. 1). The backpropagation equation can be applied repeatedly to propagate gradients through all modules, starting from the output at the top (where the network produces its prediction) all the way to the bottom (where the external input is fed). Once these gradients have been computed, it is straightforward to compute the gradients with respect to the weights of each module.

计算目标函数对多层模块的权重的梯度的反向传播过程，就是计算导数的链式法则。关键的洞见是，目标函数对一个模块的输入的导数，可以通过对这个模块的输出的导数计算出来（图1）。反向传播等式可以反复应用，将梯度传播到所有模块中，从最顶部的输出（网络得到预测结果的地方），一直到底部（外部输入的地方）。一旦这些梯度进行了计算，计算对每个模块的权重的梯度就很容易的。

Many applications of deep learning use feedforward neural network architectures (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed-size output (for example, a probability for each of several categories). To go from one layer to the next, a set of units compute a weighted sum of their inputs from the previous layer and pass the result through a non-linear function. At present, the most popular non-linear function is the rectified linear unit (ReLU), which is simply the half-wave rectifier f(z) = max(z, 0). In past decades, neural nets used smoother non-linearities, such as tanh(z) or 1/(1 + exp(−z)), but the ReLU typically learns much faster in networks with many layers, allowing training of a deep supervised network without unsupervised pre-training. Units that are not in the input or output layer are conventionally called hidden units. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1).

很多深度学习应用使用的是前向神经网络架构（图1），学习将一个固定大小的输入（如一幅图像）映射到一个固定大小的输出（比如，对每个类别的一个概率）。为从一层到下一层，一系列单元计算上一层得到的输入的加权和，将结果传输到一个非线性函数中。目前，最流行的非线性函数是ReLU函数，就是半波整流器f(z) = max(z, 0)。在过去几十年，神经网络使用更平滑的非线性函数，如tanh(z)或1/(1 + exp(−z))，但ReLU在有很多层的网络中会学的更快，这使得深度监督网络的训练不需要无监督的预训练。不在输入或输出层的单元，通常称为隐藏单元。隐含层可以视为对输入以一种非线性的方式进行扭曲，这样类别可以由最后一层变得线性可分（图1）。

In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. It was widely thought that learning useful, multistage, feature extractors with little prior knowledge was infeasible. In particular, it was commonly thought that simple gradient descent would get trapped in poor local minima — weight configurations for which no small change would reduce the average error.

在1990s后期，神经网络和反向传播已经基本被机器学习团体所抛弃，被计算机视觉和语音识别团体所忽略。不使用先验知识学习到可用的，多阶段的特征提取器，广泛认为是不可行的。特别是，通常认为，简单的梯度下降会陷入到局部极值中，在这种权重配置中，小的变化不会降低平均误差。

In practice, poor local minima are rarely a problem with large networks. Regardless of the initial conditions, the system nearly always reaches solutions of very similar quality. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general. Instead, the landscape is packed with a combinatorially large number of saddle points where the gradient is zero, and the surface curves up in most dimensions and curves down in the remainder. The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers, but almost all of them have very similar values of the objective function. Hence, it does not much matter which of these saddle points the algorithm gets stuck at.

实践中，对于大型网络来说，局部极值基本不是一个问题。不管初始条件如何，系统几乎都会得到类似质量的解。最近的理论和经验成果也都证明了，局部极值一般并不是一个严重的问题。但曲面上有非常多的鞍点，在多数维度上曲面向上，剩下的维度中曲面向下。分析似乎表明，有一些向下的方向的鞍点大量存在，但大部分的目标函数值都很类似。因此，算法卡在哪个鞍点，似乎并不重要。

Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research (CIFAR). The researchers introduced unsupervised learning procedures that could create layers of feature detectors without requiring labelled data. The objective in learning each layer of feature detectors was to be able to reconstruct or model the activities of feature detectors (or raw inputs) in the layer below. By ‘pre-training’ several layers of progressively more complex feature detectors using this reconstruction objective, the weights of a deep network could be initialized to sensible values. A final layer of output units could then be added to the top of the network and the whole deep system could be fine-tuned using standard backpropagation. This worked remarkably well for recognizing handwritten digits or for detecting pedestrians, especially when the amount of labelled data was very limited.

CIFAR的一组研究者使得人们对深度前向神经网络的兴趣得到复兴。研究者提出了无监督学习过程，可以在不需要标注数据的情况下，创建特征提取层。学习每一层特征提取器的目标，是在下面的层中，可以重建或对特征提取器的活动进行建模（或原始输入）。通过使用重建目标函数，预训练几层逐渐更加复杂的特征提取器，深度网络的权重可以初始化为有意义的值。最后的输出层单元可以放到网络最上面，整个系统可以用标准的反向传播进行精调。这对于识别手写数字，或检测行人，效果非常好，尤其是在标记数据的数量非常有限的情况。

The first major application of this pre-training approach was in speech recognition, and it was made possible by the advent of fast graphics processing units (GPUs) that were convenient to program and allowed researchers to train networks 10 or 20 times faster. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary and was quickly developed to give record-breaking results on a large vocabulary task. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting, leading to significantly better generalization when the number of labelled examples is small, or in a transfer setting where we have lots of examples for some ‘source’ tasks but very few for some ‘target’ tasks. Once deep learning had been rehabilitated, it turned out that the pre-training stage was only needed for small data sets.

这种预训练方法的一个主要应用就是语音识别，而且这由GPU的发展使其成为可能，GPU很方便进行编程，使研究者训练网络的速度快了10-20倍。在2009年，这个方法用于将从音频中提取出来的时间窗口的系数，映射到各种语言的片段的概率。其在标准语音识别基准测试中，取得了破记录的结果，使用了很小的词汇库，开发过程很快，在一个大型词汇任务中得到的破纪录的结果。在2012年，从2009开始提出了各种深度网络，而且已经部署到了Android手机上。对于更小的数据集，无监督的预训练对避免过拟合是有帮助的，在标注样本数量很小的情况下，带来了明显更好的泛化结果，在迁移学习的设置中也得到了很好的结果，其中大量样本是源任务中的，很少是目标任务中的。一旦深度学习复兴，结果发现，预训练的阶段只是对于小数据集是必须的。

There was, however, one particular type of deep, feedforward network that was much easier to train and generalized much better than networks with full connectivity between adjacent layers. This was the convolutional neural network (ConvNet). It achieved many practical successes during the period when neural networks were out of favour and it has recently been widely adopted by the computer-vision community.

但是，有一类特殊的深度前向神经网络，训练起来很容易，泛化起来效果更好。这就是卷积神经网络。其获得了很多实践上的成功，最近在计算机视觉中得到了广泛的应用。

## 3. Convolutional neural networks

ConvNets are designed to process data that come in the form of multiple arrays, for example a colour image composed of three 2D arrays containing pixel intensities in the three colour channels. Many data modalities are in the form of multiple arrays: 1D for signals and sequences, including language; 2D for images or audio spectrograms; and 3D for video or volumetric images. There are four key ideas behind ConvNets that take advantage of the properties of natural signals: local connections, shared weights, pooling and the use of many layers.

卷积网络设计的处理数据的过程为，数据是多阵列的方式，比如一幅彩色图像，有3个2D阵列，在三个色彩通道中包含像素灰度。很多数据模态都是以多阵列的形式存在的：1D是信号和序列，包括语言；2D是图像或语音频谱图；3D是视频或体的图像。在ConvNets后面，有四个关键的思想，利用了自然信号的性质：局部连接，共享权重，池化和使用很多层。

The architecture of a typical ConvNet (Fig. 2) is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a convolutional layer are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a ReLU. All units in a feature map share the same filter bank. Different feature maps in a layer use different filter banks. The reason for this architecture is twofold. First, in array data such as images, local groups of values are often highly correlated, forming distinctive local motifs that are easily detected. Second, the local statistics of images and other signals are invariant to location. In other words, if a motif can appear in one part of the image, it could appear anywhere, hence the idea of units at different locations sharing the same weights and detecting the same pattern in different parts of the array. Mathematically, the filtering operation performed by a feature map is a discrete convolution, hence the name.

一个典型的ConvNet的架构（图2），是多个阶段的结构。前面几个阶段，是由两种类型的层组成的：卷积层和池化层。卷积层的单元，是以特征图的形式组织起来的，其中每个单元都通过一系列权重集，称为滤波器组，与上一层的特征图的局部块相联。这种局部加权和的结果，然后通过一个非线性单元，如ReLU。一个特征图中的所有单元，都共享同样的滤波器组。一个层中不同的特征图，使用不同的滤波器组。这种架构的原因是两重的。第一，在图像这样的阵列数据中，局部组的值一般是高度相关的，形成了有区分性的局部图样，可以很容易的被检测到。第二，图像的局部统计值和其他信号对于位置是不变的。换句话说，如果一个图样可以在图像的一个位置出现，那么就可以出现在任何地方，因此有了这样的思想，即在不同位置共享同样权重的单元，在阵列的不同位置检测同样的模式。数学上来说，特征图进行的滤波的运算，是一个离散卷积，因此是相同的。

Although the role of the convolutional layer is to detect local conjunctions of features from the previous layer, the role of the pooling layer is to merge semantically similar features into one. Because the relative positions of the features forming a motif can vary somewhat, reliably detecting the motif can be done by coarse-graining the position of each feature. A typical pooling unit computes the maximum of a local patch of units in one feature map (or in a few feature maps). Neighbouring pooling units take input from patches that are shifted by more than one row or column, thereby reducing the dimension of the representation and creating an invariance to small shifts and distortions. Two or three stages of convolution, non-linearity and pooling are stacked, followed by more convolutional and fully-connected layers. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.

虽然卷积层的角色是检测上一层的特征的局部连接，但池化层的角色是将语义上类似的特征融合成一个。因为形成一个模式的特征的相对位置可以在某种程度上变化，模式的可靠检测，可以通过对每个特征的位置进行粗粒化进行解决。典型的池化单元，计算特征值一个局部块的最大值（或者是几个特征图中）。邻域的池化单元的输入，是偏移了一行或一列的特征图块，因此减少了表示的维度，创建了对小的平移和变形的不变性。两个或三个阶段的卷积，非线性和池化堆叠在一起，然后与更多的卷积和全连接层一起。通过卷积层进行反向传播梯度，与通过常规的深度网络，是一样简单的，使得所有滤波器组中的所有权重都得到训练。

Deep neural networks exploit the property that many natural signals are compositional hierarchies, in which higher-level features are obtained by composing lower-level ones. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. The pooling allows representations to vary very little when elements in the previous layer vary in position and appearance.

深度神经网络利用的性质的是，很多神经信号是组合的层次结构的，其中较高层次的特征是通过组合较低层得到的。在图像中，边缘的局部组合形成了图样，图样组合成了部位，不同部位的组合形成了目标。在语音和文本中也有类似的层次结构，从声音到电话，音素，音节，单词和语句。这种汇集使得，前一层的元素在位置和外观上变化时，这一层的表示变化很小。

The convolutional and pooling layers in ConvNets are directly inspired by the classic notions of simple cells and complex cells in visual neuroscience, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway. When ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex. ConvNets have their roots in neocognitron, the architecture of which was somewhat similar, but did not have an end-to-end supervised-learning algorithm such as backpropagation. A primitive 1D ConvNet called a time-delay neural net was used for the recognition of phonemes and simple words.

ConvNets中的卷积和池化层，是受到视觉神经科学中的简单细胞和复杂细胞的经典概念所启发的，其总体架构让人想起视觉皮层路径中的LGN–V1–V2–V4–IT层次结构。当给ConvNet模型和猴子看同样的图像时，ConvNet中高层单元的激活，解释了猴子的颞下皮层的160个神经元的随机集合的一半的变化。ConvNets在神经认知学中有其根基，其架构与一些类似，但并没有一个端到端的监督学习的算法，比如反向传播。1D ConvNet的原型，称为延时神经网络，用在了音素和简单单词的识别中。

There have been numerous applications of convolutional networks going back to the early 1990s, starting with time-delay neural networks for speech recognition and document reading. The document reading system used a ConvNet trained jointly with a probabilistic model that implemented language constraints. By the late 1990s this system was reading over 10% of all the cheques in the United States. A number of ConvNet-based optical character recognition and handwriting recognition systems were later deployed by Microsoft. ConvNets were also experimented with in the early 1990s for object detection in natural images, including faces and hands, and for face recognition.

ConvNets在1990s就有很多应用，从语音识别和文档阅读的延时神经网络开始。文档阅读系统，使用了一个ConvNet，与一个实现了语言限制的概率模型一同进行训练。在1990s下半期，这个系统阅读了全美10%的支票。后来微软开发并部署了几个基于ConvNet的OCR和手写字体识别系统。ConvNets在1990s早期还进行了自然图像目标检测的试验，包括人脸和手，以及人脸识别。

## 4. Image understanding with deep convolutional networks

Since the early 2000s, ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images. These were all tasks in which labelled data was relatively abundant, such as traffic sign recognition, the segmentation of biological images particularly for connectomics, and the detection of faces, text, pedestrians and human bodies in natural images. A major recent practical success of ConvNets is face recognition.

自从2000s早期，Convnets已经很成功的应用于图像中目标和区域的检测，分割和识别。这些任务中，标记的数据都相对非常充裕，比如交通标志识别，生物图像的分割，尤其是在连接组学中，以及自然图像中人脸，文本，行人和人体的检测。最近ConvNet的一个实际成功，在人脸识别中。

Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Companies such as Mobileye and NVIDIA are using such ConvNet-based methods in their upcoming vision systems for cars. Other applications gaining importance involve natural language understanding and speech recognition.

更重要的是，在像素层次进行标注的图像，会在很多技术中都有应用，包括自动驾驶机器人和自动驾驶汽车。像Mobileye和NVIDIA这样的公司，在正在开发的车辆视觉系统中，正在使用基于ConvNet的方法。其他越来越重要的应用包括自然语言理解和语音识别。

Despite these successes, ConvNets were largely forsaken by the mainstream computer-vision and machine-learning communities until the ImageNet competition in 2012. When deep convolutional networks were applied to a data set of about a million images from the web that contained 1,000 different classes, they achieved spectacular results, almost halving the error rates of the best competing approaches. This success came from the efficient use of GPUs, ReLUs, a new regularization technique called dropout, and techniques to generate more training examples by deforming the existing ones. This success has brought about a revolution in computer vision; ConvNets are now the dominant approach for almost all recognition and detection tasks and approach human performance on some tasks. A recent stunning demonstration combines ConvNets and recurrent net modules for the generation of image captions (Fig. 3).

尽管有这些成功，ConvNets直到2012年，一直不被主流计算机视觉和机器学习团体所使用。当ConvNets应用于一个百万幅图像的数据集，从网络上收集的，包含1000个不同的类别，这取得了非常好的结果，几乎是最好的方法的错误率减半。这种成功来源于GPU，ReLU，一种新的正则化技术，称为dropout，和对现有的训练样本进行变化，以生成更多的技术的应用。其成功已经带来了计算机视觉技术的革命；ConvNets现在几乎是所有识别和检测任务的主流方法，在一些任务中接近人类的性能。最近ConvNets和RNN模块结合到一起，生成图像标题，其演示非常惊人（图3）。

Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. Whereas training such large networks could have taken weeks only two years ago, progress in hardware, software and algorithm parallelization have reduced training times to a few hours.

最近的ConvNets架构有10-20层ReLUs，上亿个权重，在神经元之间有数十亿个连接。而训练这样的大型网络在两年之前，可能需要几个星期，在硬件、软件和算法上的并行发展，已经使训练时间缩短到了几个小时。

The performance of ConvNet-based vision systems has caused most major technology companies, including Google, Facebook, Microsoft, IBM, Yahoo!, Twitter and Adobe, as well as a quickly growing number of start-ups to initiate research and development projects and to deploy ConvNet-based image understanding products and services.

基于ConvNet的视觉算法的性能，已经使得主要的科技公司，包括Google, Facebook, Microsoft, IBM, Yahoo!, Twitter和Adobe，以及数量迅速增加的初创公司，开发并部署基于ConvNet的图像理解产品和服务。

ConvNets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays. A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.

ConvNet很容易在芯片和FPGA上进行高效的硬件实现。几个公司，如NVIDIA, Mobileye, Intel, Qualcomm和Samsung，在开发ConvNet芯片，以开发手机、相机、机器人和自动驾驶汽车的实时视觉应用。

## 5. Distributed representations and language processing

Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure. First, learning distributed representations enable generalization to new combinations of the values of learned features beyond those seen during training (for example, 2^n combinations are possible with n binary features). Second, composing layers of representation in a deep net brings the potential for another exponential advantage (exponential in the depth).

深度学习理论表明，深度网络与经典学习算法相比，有两个不同的优势，因为经典学习算法不使用分布式表示。这两个优势都是组合的能力带来的，依赖于潜在的数据生成分布，存在合适的组合结构。第一，学习分布式表示可以泛化到学习的特征的新的组合，这些都是在训练的时候未看到的（比如，n个二值特征可能有2^n的组合）。第二，将深度网络中分层的表示组合到一起，带来了另一个可能的优势（在深度上的）。

The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. This is nicely demonstrated by training a multilayer neural network to predict the next word in a sequence from a local context of earlier words. Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. In the first layer, each word creates a different pattern of activations, or word vectors (Fig. 4). In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word, which can be used to predict the probability for any word in the vocabulary to appear as the next word. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word, as was first demonstrated in the context of learning distributed representations for symbols. These semantic features were not explicitly present in the input. They were discovered by the learning procedure as a good way of factorizing the structured relationships between the input and output symbols into multiple ‘micro-rules’. Learning word vectors turned out to also work very well when the word sequences come from a large corpus of real text and the individual micro-rules are unreliable. When trained to predict the next word in a news story, for example, the learned word vectors for Tuesday and Wednesday are very similar, as are the word vectors for Sweden and Norway. Such representations are called distributed representations because their elements (the features) are not mutually exclusive and their many configurations correspond to the variations seen in the observed data. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network. Vector representations of words learned from text are now very widely used in natural language applications.

多层神经网络的隐藏层，学习对网络的输入进行一种表示，这样预测目标结果会比较容易。这在一个应用中证明的很好，即训练一个多层神经网络，从一个序列中的之前的单词的局部上下文中，预测下一个单词。这个上下文的每个单词，都以N维向量送入网络，向量的其中一个元素为1，其余为0。在第一层，每个单词创建了一种不同的激活模式，或单词向量（图4）。在一个语言模型中，网络的其他层学习将输入单词向量转换到输出单词向量，以预测下一个单词，这可以用于预测词汇表中任意单词出现为下一个单词的概率。网络学习的单词向量，包含了很多活跃的元素，其中每个都可以解释成单词的单独未来，这首次是在学习符号的分布式表示的上下文中所展现出来的。这些语义特征在输入中并不是显式存在的。它们是通过学习过程发现的，将输入和输出符号之间的结构化的关系，分解成很多微规则。学习单词向量，在单词序列是来自于真实语料时，也会表现很好，而个体的微规则则是不可靠的。比如，当被训练在新闻故事中预测下一个单词时，学习到的星期二和星期三的单词向量是非常类似的，对于瑞典和挪威的单词向量也是的。这种表示称之为分布式表示，因为其元素（特征）并不是互斥的，它们的很多配置对应的是观察数据中的变化。这些单词向量是由学习到的特征组成的，并不是由专家事先确定的，而是由神经网络自动发现的。由文本学习到的单词的向量表示，现在在自然语言处理中应用非常广泛。

The issue of representation lies at the heart of the debate between the logic-inspired and the neural-network-inspired paradigms for cognition. In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbol instances. It has no internal structure that is relevant to its use; and to reason with symbols, they must be bound to the variables in judiciously chosen rules of inference. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning.

在逻辑启发的和神经网络启发的范式进行识别之间，表示的问题是在论辩的核心。在逻辑启发的范式中，一个符号的实例，其属性是与其他符号实例相比，要么是完全一致的，要么是不一致的。它没有与其使用相关的内部结构；要用符号进行推理的话，它们必须与审慎的选择的推理规则相绑定。相比之下，神经网络使用的只是大型活动向量，大型权重矩阵和标量的非线性，以进行快速的直觉性的推理，这些是容易的常识推理的支撑。

Before the introduction of neural language models, the standard approach to statistical modelling of language did not exploit distributed representations: it was based on counting frequencies of occurrences of short symbol sequences of length up to N (called N-grams). The number of possible N-grams is on the order of V^N, where V is the vocabulary size, so taking into account a context of more than a handful of words would require very large training corpora. N-grams treat each word as an atomic unit, so they cannot generalize across semantically related sequences of words, whereas neural language models can because they associate each word with a vector of real valued features, and semantically related words end up close to each other in that vector space (Fig. 4).

在神经语言模型引入之前，语言统计模型的标准方法并没有利用分布式表示：是基于长度最多为N的短符号序列出现的频率的（称为N-grams）。可能的N-grams的数量是V^N级的，其中V是词汇表的大小，所以考虑多于几个单词的上下文，会需要很大的训练语料库。N-grams将每个单词都视为一个原子单元，所以它们不能在语义相关的单词序列之间进行泛化，而神经语言模型可以，因为它们将每个单词都用一个实值特征向量进行相关了，而语义相关的单词在向量空间中会很接近（图4）。

## 6. Recurrent neural networks

When backpropagation was first introduced, its most exciting use was for training recurrent neural networks (RNNs). For tasks that involve sequential inputs, such as speech and language, it is often better to use RNNs (Fig. 5). RNNs process an input sequence one element at a time, maintaining in their hidden units a ‘state vector’ that implicitly contains information about the history of all the past elements of the sequence. When we consider the outputs of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network (Fig. 5, right), it becomes clear how we can apply backpropagation to train RNNs.

反向传播首次提出时，其最令人激动的应用是，用于训练循环神经网络(RNNs)。对于涉及到顺序输入的任务，比如语音和语言，使用RNNs会更好一些。RNNs处理输入序列时，一次处理一个元素，在隐藏单元中维护一个状态向量，隐式的包含了序列所有过去元素的历史的信息。当我们在不同的离散时间步骤时，考虑隐藏单元的输出，好像他们是深度多层网络的不同神经元的输出（图5，右），我们怎样应用反向传播来训练RNNs，就很清楚了。

RNNs are very powerful dynamic systems, but training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish.

RNNs对于动态系统非常好用，但训练的时候问题很多，因为反向传播的梯度每次都在增长或缩小，所以在很多步骤之后，通常都会有梯度爆炸或梯度消失的问题。

Thanks to advances in their architecture and ways of training them, RNNs have been found to be very good at predicting the next character in the text or the next word in a sequence, but they can also be used for more complex tasks. For example, after reading an English sentence one word at a time, an English ‘encoder’ network can be trained so that the final state vector of its hidden units is a good representation of the thought expressed by the sentence. This thought vector can then be used as the initial hidden state of (or as extra input to) a jointly trained French ‘decoder’ network, which outputs a probability distribution for the first word of the French translation. If a particular first word is chosen from this distribution and provided as input to the decoder network it will then output a probability distribution for the second word of the translation and so on until a full stop is chosen. Overall, this process generates sequence of French words according to a probability distribution that depends on the English sentence. This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art, and this raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies that each contribute plausibility to a conclusion.

有了RNNs的架构和训练他们的方法，RNNs很擅长于预测文本中的下一个字符或序列中的下一个单词，但也可以用于更复杂的任务。比如，在一次一个单词阅读一个英语语句后，可以训练一个英文编码器网络，这样其隐藏单元的状态向量是这个语句所表达的思想的很好的表示。这个思想向量可以用于一个联合训练的法语编码器网络的初始隐藏状态（或用作额外输入），这个网络输出的是法语翻译的第一个单词的概率分布。如果从这个分布中选择一个特定的第一单词，作为解码器网络输入，然后就会输出第二个单词的翻译的概率分布，这样持续进行下去，直到选择了一个结束。总体上，这个过程生成的法语单词序列，是根据依赖于英文语句的一个概率分布。这种颇为天然的进行机器翻译的方式，已经迅速的成为很有竞争力的目前最好的结果，这带来了极大的怀疑，理解一个语句是否需要任何像内部符号表示的东西，这通常是用推理规则得到的。这与下面的观点颇为吻合，日常推理涉及到很多并发的类比，每个都对结论的可行性有所贡献。

Instead of translating the meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image into an English sentence (Fig. 3). The encoder here is a deep ConvNet that converts the pixels into an activity vector in its last hidden layer. The decoder is an RNN similar to the ones used for machine translation and neural language modelling. There has been a surge of interest in such systems recently (see examples mentioned in ref. 86).

如果不是将法语语句翻译成英语语句，那还可以将一幅图像的意义翻译成一个英语语句（图3）。这里的编码器是一个深度神经网络，将像素在其最后的隐含层中转换成一个行为向量。解码器是一个RNN，与机器翻译和神经语言建模使用的类似。这种系统的兴趣最近很多（见参考文献86中的例子）。

RNNs, once unfolded in time (Fig. 5), can be seen as very deep feedforward networks in which all the layers share the same weights. Although their main purpose is to learn long-term dependencies, theoretical and empirical evidence shows that it is difficult to learn to store information for very long.

RNNs，一旦在时间上进行展开（图5），可以看作是非常深的前馈网络，其中所有层都共享同样的权重。虽然其主要目的是学习长期依赖性，理论和经验证据都表明，要学习存储长期信息是很困难的。

To correct for that, one idea is to augment the network with an explicit memory. The first proposal of this kind is the long short-term memory (LSTM) networks that use special hidden units, the natural behaviour of which is to remember inputs for a long time. A special unit called the memory cell acts like an accumulator or a gated leaky neuron: it has a connection to itself at the next time step that has a weight of one, so it copies its own real-valued state and accumulates the external signal, but this self-connection is multiplicatively gated by another unit that learns to decide when to clear the content of the memory.

为对这个进行修正，一个想法是，用显式的存储对网络进行扩充。第一个这种建议是LSTM网络，使用了特殊的隐藏层，其自然行为是在很长时间内记住输入。一个特殊单元，称为记忆单元，行为与累加器或门控泄漏神经元类似：其与本身有连接，在下一次步骤中，权重为1，所以将其本身的实值状态拷贝并累积了外部信号，但这种自连接被另一个单元进行了多次门控，以学习决定什么时候对存储的内容进行清空。

LSTM networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription. LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation.

LSTM网络后来证明比传统RNNs更加高效，尤其是在每个步骤中有几层，使整个语音识别系统从语音直接到字符序列。LSTM网络或相关形式的门控单元，目前也用作编码器和解码器网络，在机器翻译中表现很好。

Over the past year, several authors have made different proposals to augment RNNs with a memory module. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to, and memory networks, in which a regular network is augmented by a kind of associative memory. Memory networks have yielded excellent performance on standard question-answering benchmarks. The memory is used to remember the story about which the network is later asked to answer questions.

在过去这些年，几位作者提出了不同的方法，用存储模块来扩充RNNs。提议包括Neural Turing Machine，其中网络由一个类似磁带的存储进行扩充，RNN可以选择从中读取或写入内容，以及memory networks，其中一个常规网络由关联存储进行扩充。Memory networks已经在标准的问答基准测试中得到了很好的性能。其存储用于记住后面网络要被询问的故事。

Beyond simple memorization, neural Turing machines and memory networks are being used for tasks that would normally require reasoning and symbol manipulation. Neural Turing machines can be taught ‘algorithms’. Among other things, they can learn to output a sorted list of symbols when their input consists of an unsorted sequence in which each symbol is accompanied by a real value that indicates its priority in the list. Memory networks can be trained to keep track of the state of the world in a setting similar to a text adventure game and after reading a story, they can answer questions that require complex inference. In one test example, the network is shown a 15-sentence version of the The Lord of the Rings and correctly answers questions such as “where is Frodo now?”

除了简单的记忆，neural Turing machines和memory network还用于其他任务，一般需要推理和符号计算的。可以教Neural Turing machines一些算法。在其他事情中，当输入为未排序的序列时，每个序列都伴有一个实值，表明其在列表中的优先级，它们可以学习输出符号排序列表。Memory networks可以训练来追踪世界的状态，其设置与文本冒险游戏类似，在读取一个故事后，可以回答需要复杂推理的问题。在一个测试例子中，网络经过一个15个语句的指环王的故事的训练，准确的回答出了像“where is Frodo now?”这样的问题。

## 7. The future of deep learning

Unsupervised learning had a catalytic effect in reviving interest in deep learning, but has since been overshadowed by the successes of purely supervised learning. Although we have not focused on it in this Review, we expect unsupervised learning to become far more important in the longer term. Human and animal learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object.

无监督学习在深度学习兴趣的复兴中有类似催化剂的作用，但一直被纯监督学习的成功所掩盖。虽然我们在本文中没有聚焦在此，我们期待在长期来看，无监督学习会变得更加重要。人类和动物学习主要都是无监督的：我们发现这个世界的结构主要是通过观察，而并不是被告知每个目标的名字。

Human vision is an active process that sequentially samples the optic array in an intelligent, task-specific way using a small, high-resolution fovea with a large, low-resolution surround. We expect much of the future progress in vision to come from systems that are trained end-to-end and combine ConvNets with RNNs that use reinforcement learning to decide where to look. Systems combining deep learning and reinforcement learning are in their infancy, but they already outperform passive vision systems at classification tasks and produce impressive results in learning to play many different video games.

人类视觉是一个主动过程，对光学阵列进行序列取样，其方式是智能的，与任务相关的，使用的是一个小的、高分辨率的中央凹，和大的低分辨率的周围环境。我们期待未来的视觉进展，主要来自端到端的训练的系统，将ConvNets和RNNs结合到一起，使用强化学习来决定向哪里看。将深度学习和强化学习结合到一起的系统，仍然非常原始，但已经在分类任务中超越了被动视觉系统，在学习进行很多视频游戏中，产生了令人印象深刻的结果。

Natural language understanding is another area in which deep learning is poised to make a large impact over the next few years. We expect systems that use RNNs to understand sentences or whole documents will become much better when they learn strategies for selectively attending to one part at a time.

自然语言理解，是另一个领域，其中深度学习在未来几年中会取得很大影响。我们期待使用RNNs理解语句或整篇文档的系统，当学习一次选择性参与一部分的时候，效果会好的多。

Ultimately, major progress in artificial intelligence will come about through systems that combine representation learning with complex reasoning. Although deep learning and simple reasoning have been used for speech and handwriting recognition for a long time, new paradigms are needed to replace rule-based manipulation of symbolic expressions by operations on large vectors.

最后，人工智能中的主要进展会通过将表示学习与复杂推理相结合的系统展现出来。虽然深度学习和简单的推理已经在很长时间内用于语音和手写字体识别，但需要新的范式，来替换掉基于规则的符号表示的操作，对大型向量进行运算。